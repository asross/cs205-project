One main thing we would attempt to determine next would be the (cost or time) optimal \\(\epsilon\\) for a given distribution and set of parallel resources. We have demonstrated that, at the very least, neither \\(\epsilon=0\\) nor \\(\epsilon=1\\) is optimal for a high dimensional Gaussian mixture model, but we have not found the best value.

Another complexity that should be analyzed is how much our method of parallelizing rejection sampling (within each MPI node) affects this optimal \\(\epsilon\\). Because of this strong coupling between parallel scaling and \\(\epsilon\\), improving the parallelism of our algorithm could potentially mean that we must choose a higher value if we want to reach a specified convergence threshold in a cost-optimal manner.

Finally, we would like to shore up our theoretical analysis of the relationship between the variance of the distribution and the scaling properties of samplers. In general, we think there is value in trying to bridge the conceptual gaps between parallel computing and statistics, optimization, and numerical methods, which are subjects we have all been studying "in parallel" in the CSE program. We hope that by sampling all of these academic modes together, with plenty of jumping back and forth, we can ultimately converge on better solutions to the big science problems that motivate them all.
