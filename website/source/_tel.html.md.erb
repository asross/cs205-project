Our main intuition is to simply linearly interpolate between rejection sampling
and MCMC. There is related work (<%= cite 'wormhole' %>, <%= cite 'darting' %>)
which attempts to (literally) bridge some of the gaps in MCMC, but to our
knowledge, our particular method is novel -- though perhaps only because
rejection sampling is tricky to set up and becomes inefficient so quickly.

Nevertheless, in our method, during each Metropolis-Hastings step, we either
sample from our proposal distribution with probability \\(1-\epsilon\\) (in
which case we accept or reject using the normal MH acceptance probability) or
we “teleport” to a new location obtained by rejection sampling with probability
\\(\epsilon\\). Psuedocode is as follows:


<div class='well' style='font-family: monospace; white-space: pre'><strong>function</strong> TeleportingMCMC(\(s\), \(p\), \(x_0\), N, \(\epsilon\)):
  samples = [\(x_0\)]
  attempts = 0
  <strong>while</strong> attempts < N:
    <strong>if</strong> rand(0,1) < \(\epsilon\):
      <strong>repeat</strong> {
        \(x, y\) \(\sim\) box enclosing \(s\)
        attempts++
      } <strong>until</strong> y < \(s(x)\)
      samples.add(\(x\))
    <strong>else</strong>:
      \(x_{1}\) = previous sample
      \(x_{2} \sim p(x|x_{1})\)
      attempts++
      samples.add(rand(0,1) < \(s(x_{2})/s(x_{1})\) ? \(x_{2}\) : \(x_{1}\))
  <strong>return</strong> samples
</div>

It is straightforward to show that this scheme, expressed
within the framework of traditional Metropolis-Hastings, still satisfies the detailed
balance criteria:

$$
\frac{\text{prob of }x_1\to x_2}{\underbrace{\text{prob of }x_2\to x_1}_{\text{must eq. $s(x_1)/s(x_2)$}}} = \frac{(1-\epsilon)p(x_1|x_2)s(x_1) + (\epsilon) s(x_1)}{\underbrace{(1-\epsilon)p(x_2|x_1)s(x_2)}_{\text{proposal}} + \underbrace{(\epsilon) s(x_2)}_{\text{teleport}}} = \frac{
    \big(p(x_1|x_2)(1-\epsilon) + \epsilon\big)s(x_1)
}{
  \big( \underbrace{p(x_2|x_1)}_{=\, p(x_1|x_2)}(1-\epsilon) + \epsilon\big) s(x_2)
  } = \underbrace{\frac{s(x_1)}{s(x_2)}}_{\text{balanced}}
$$

Within this framework, setting \\(\epsilon=0\\) is exactly Metropolis-Hastings,
while \\(\epsilon=1\\) is exactly rejection sampling. 

What we would like to investigate is whether, by choosing an intermediate value
for \\(\epsilon\\), we can obtain samples that allow us to reach convergence
more quickly with better parallel scaling.

We've animated the difference between \\(\epsilon=0\\) and \\(\epsilon=.0334\\) here:

![anim](mcmc-tel-animation.gif)
