Many of the biggest science applications running today -- from climate models to economic forecasting to quantum chemistry -- are probabilistic simulations. Much of this big science relies on Monte-Carlo methods (basically, drawing lots of samples and taking averages) because the models involved are often too complex, with too many intermediate branches, to deal with analytically. Complexity, especially probabilistic complexity, is a subtle but ubiquitous "bigness" of data with which science must constantly contend.

A specific problem (related to the familiar idea of nonconvexity) is multimodality. When a probabilistic simulation is multimodal, there isn't a single cluster of likely configurations centered around a characteristic average. Instead, there may be multiple such clusters distributed far away from each other in the model's parameter space. One example where this occurs is topic modeling <%= %>, and also in the problem of localizing sensors based on noisy measurements of intersensor distance -- which is commonly used as a benchmark for multimodal sampling techniques <%= cite 'wormhole' %>, <%= cite 'darting' %>. 

More stuff
