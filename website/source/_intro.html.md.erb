Many of the biggest science applications running today -- from climate models <%= cite 'climate' %> to economic forecasting <%= cite 'forecast' %> to experimental physics <%= cite 'physics' %> -- are probabilistic simulations. Because the models involved are often too complex to integrate analytically, these models generally rely on Monte Carlo methods (basically, drawing lots of samples and taking averages) to make predictions. But even sampling from these simulations can be challenging. To do big science, practitioners must not only contend with increasing data size but also increasing problem complexity.

A specific complexity we are concerned with here (related to the familiar idea of non-convexity) is multimodality. When a probabilistic simulation is multimodal, there isn't a single cluster of likely configurations centered around a characteristic average. Instead, there are multiple such clusters distributed far away from each other in the model's parameter space. One example where this occurs is topic modeling <%= cite 'topicmodes' %>, and also in the problem of localizing sensors based on noisy measurements of inter-sensor distance -- which is commonly used as a benchmark for multimodal sampling techniques <%= cite 'wormhole' %>, <%= cite 'darting' %>.

In this project, we will consider how parallelism can help us deal with problems of big statistical complexity that arise in such applications. We will also introduce a novel method for efficiently sampling from multimodal distributions, analyze its scaling properties, and present our implementation using state-of-the-art parallel computing technologies.
